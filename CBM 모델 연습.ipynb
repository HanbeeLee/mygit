{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d805084",
   "metadata": {},
   "source": [
    "### 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c647460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# device ì„¤ì •\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6105bf1",
   "metadata": {},
   "source": [
    "### 2. ë°ì´í„° ìƒì„± ë° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f933330",
   "metadata": {},
   "source": [
    "#### ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a97b839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ê°€ìƒ ë°ì´í„° ìƒì„± (ì •ìƒ ë°ì´í„°ëŠ” ì‚¬ì¸íŒŒ, ì´ìƒì¹˜ëŠ” ê¸‰ê²©í•œ ë…¸ì´ì¦ˆ)\n",
    "t = np.linspace(0, 100, 1000)\n",
    "normal_signal = np.sin(t).reshape(-1, 1)\n",
    "\n",
    "# Train: ì •ìƒ ë°ì´í„°ë§Œ ì¡´ì¬\n",
    "train_data = normal_signal[:800] \n",
    "\n",
    "# Test: ì •ìƒ ë°ì´í„° + ê°•í•œ ì´ìƒì¹˜(Outliers) ì‚½ì…\n",
    "test_data = normal_signal[800:].copy()\n",
    "test_data[50:60] += 10.0  # ê°•ë ¥í•œ ì´ìƒì¹˜ ì‹ í˜¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a302f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ì˜ˆì‹œ ë°ì´í„° ìƒì„±\n",
    "df = pd.DataFrame({\n",
    "    'ì´ë¦„': ['A', 'B', 'C', 'A', 'E', 'F', 'G'],\n",
    "    'ì ìˆ˜': [10, 20, 30, 40, 50, 60, 70],\n",
    "})\n",
    "\n",
    "# 2. 'ì´ë¦„' ì—´ì—ì„œ 'A'ê°€ ë§ˆì§€ë§‰ìœ¼ë¡œ ë“±ì¥í•˜ëŠ” ì¸ë±ìŠ¤ ì°¾ê¸°\n",
    "# (ë°ì´í„°ì— 'A'ê°€ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)\n",
    "target_name = 'A'\n",
    "indices = df[df['ì´ë¦„'] == target_name].index\n",
    "\n",
    "if not indices.empty:\n",
    "    # ë§ˆì§€ë§‰ ì¸ë±ìŠ¤ ì¶”ì¶œ\n",
    "    last_index = indices[-1]\n",
    "    \n",
    "    # í•´ë‹¹ ìœ„ì¹˜ë¶€í„° ëê¹Œì§€ ìŠ¬ë¼ì´ì‹±\n",
    "    df_result = df.iloc[last_index:]\n",
    "\n",
    "    # 3. CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "    # index=False: 0, 1, 2 ê°™ì€ í–‰ ë²ˆí˜¸ëŠ” íŒŒì¼ì— ì €ì¥í•˜ì§€ ì•ŠìŒ\n",
    "    # encoding='utf-8-sig': ì—‘ì…€ì—ì„œ ì—´ì—ˆì„ ë•Œ í•œê¸€ ê¹¨ì§ ë°©ì§€\n",
    "    df_result.to_csv('result_data.csv', index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(\"ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    print(df_result)\n",
    "else:\n",
    "    print(f\"'{target_name}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ì–´ ì €ì¥í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5e5c6a",
   "metadata": {},
   "source": [
    "ğŸ’¡ ì €ì¥ ì˜µì…˜ ì„¤ëª…\n",
    "'result_data.csv': ì €ì¥í•  íŒŒì¼ ì´ë¦„ì…ë‹ˆë‹¤. ì›í•˜ëŠ” ê²½ë¡œê°€ ìˆë‹¤ë©´ 'C:/folder/result.csv' ì²˜ëŸ¼ ì „ì²´ ê²½ë¡œë¥¼ ì ì–´ì£¼ì…”ë„ ë©ë‹ˆë‹¤.\n",
    "\n",
    "index=False: ë°ì´í„°í”„ë ˆì„ ì™¼ìª½ì— ë¶™ëŠ” ìˆ«ì(0, 1, 2...)ë¥¼ ì œì™¸í•˜ê³  ì‹¤ì œ ë°ì´í„°ë§Œ ì €ì¥í•©ë‹ˆë‹¤. ë³´í†µ ê²°ê³¼ë¬¼ íŒŒì¼ì—ëŠ” í¬í•¨í•˜ì§€ ì•ŠëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.\n",
    "\n",
    "encoding='utf-8-sig': íŠ¹íˆ í•œêµ­ì–´ ìœˆë„ìš° í™˜ê²½ì˜ ì—‘ì…€ì—ì„œ CSV íŒŒì¼ì„ ì—´ ë•Œ í•œê¸€ì´ ê¹¨ì§€ëŠ” ê²ƒì„ ë§‰ì•„ì£¼ëŠ” ê°€ì¥ í™•ì‹¤í•œ ì¸ì½”ë”© ë°©ì‹ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì´ì œ ì´ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ ì‹¤í–‰ ê²½ë¡œì— result_data.csv íŒŒì¼ì´ ìƒì„±ë©ë‹ˆë‹¤! ì¶”ê°€ë¡œ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f376442d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# HW stateì—ì„œ ë³¼ SVID\n",
    "ctrl_hw = {'heater':[], 'gauge':[]}\n",
    "nonctrl_hw = {}\n",
    "\n",
    "# Process stateì—ì„œ ë³¼ SVID\n",
    "ctrl_process = {} # step_stateì— ë”°ë¼ í•™ìŠµí•  setì´ ë°”ë€œ (stepì„ ì§€ì •í•˜ê³  ê·¸ stepì—ì„œë§Œ í•™ìŠµí•¨)\n",
    "nonctrl_process = {} \n",
    "\n",
    "\n",
    "# df ì¤‘ ì§€ì •í•œ SVID(ì»¬ëŸ¼ëª…)ì¤‘ì—ì„œ step_state.value ê°™ì€ ê²ƒ ë¼ë¦¬ ë¬¶ìŒ\n",
    "# ê·¸ëŸ¼ dfë¥¼ 2ê°€ì§€ë¡œ ë‚˜ëˆ ì•¼í•¨ ìƒì‹œì¸ê±°ì™€ ì•„ë‹Œê±°\n",
    "# ê·¸ë¦¬ê³  ë‚˜ì„œ dfì¤‘ df_stateë¥¼ step_state.valueë³„ë¡œ ë”°ë¡œë”°ë¡œ ìƒì„±\n",
    "\n",
    "for i in df:\n",
    "    # 'ì†Œì†' ì—´ì˜ ê°’ì´ ê°™ì€ ê²ƒë¼ë¦¬ ë¬¶ì–´ì„œ ë”•ì…”ë„ˆë¦¬ì— ì €ì¥\n",
    "    step = {name: group for name, group in df.groupby('step_no')}\n",
    "\n",
    "   \n",
    "# í™•ì¸í•˜ê¸°\n",
    "print(step['1'])\n",
    "\n",
    "\n",
    "step_state = {\n",
    "    9:[],\n",
    "    10:[],\n",
    "    11:[],\n",
    "    12:[],\n",
    "    13:[],\n",
    "    14:[],\n",
    "    15:[],\n",
    "    16:[],\n",
    "    17:[],\n",
    "    18:[],\n",
    "    19:[],\n",
    "    20:[],\n",
    "    21:[],\n",
    "    22:[],\n",
    "    23:[],\n",
    "    24:[],\n",
    "    25:[],\n",
    "    26:[]\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77ade4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ì´ë¦„  ì†Œì†  ì ìˆ˜\n",
      "0  Sora  AíŒ€  90\n",
      "2  Jane  AíŒ€  95\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ì˜ˆì‹œ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "df = pd.DataFrame({\n",
    "    'ì´ë¦„': ['Sora', 'Minho', 'Jane', 'Kevin', 'Lia'],\n",
    "    'ì†Œì†': ['AíŒ€', 'BíŒ€', 'AíŒ€', 'CíŒ€', 'BíŒ€'],\n",
    "    'ì ìˆ˜': [90, 85, 95, 80, 70]\n",
    "})\n",
    "\n",
    "# 'ì†Œì†' ì—´ì˜ ê°’ì´ ê°™ì€ ê²ƒë¼ë¦¬ ë¬¶ì–´ì„œ ë”•ì…”ë„ˆë¦¬ì— ì €ì¥\n",
    "new_dfs = {name: group for name, group in df.groupby('ì†Œì†')}\n",
    "\n",
    "# í™•ì¸í•˜ê¸°\n",
    "print(new_dfs['AíŒ€'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5d5643a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: ['A', 'C'], 2: ['B', 'E'], 3: ['D']}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "data = {'A': 1, 'B': 2, 'C': 1, 'D': 3, 'E': 2}\n",
    "\n",
    "new_data = defaultdict(list)\n",
    "for key, value in data.items():\n",
    "    new_data[value].append(key)\n",
    "\n",
    "# ì¼ë°˜ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (í•„ìš”í•œ ê²½ìš°)\n",
    "result = dict(new_data)\n",
    "print(result)\n",
    "# ê²°ê³¼: {1: ['A', 'C'], 2: ['B', 'E'], 3: ['D']}\n",
    "\n",
    "# P2, Zr, Hf, O3\n",
    "#  0   1   2   3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8fe6b4",
   "metadata": {},
   "source": [
    "# Factory Method\n",
    "\n",
    "class Pizza: pass\n",
    "class CheesePizza(Pizza): pass\n",
    "class PepperoniPizza(Pizza): pass\n",
    "\n",
    "def create_pizza(kind):\n",
    "    if kind == \"cheese\":\n",
    "        return CheesePizza()\n",
    "    elif kind == \"pepperoni\":\n",
    "        return PepperoniPizza()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "p = create_pizza(\"cheese\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d45a2b",
   "metadata": {},
   "source": [
    "# Abstract Factory\n",
    "\n",
    "# ì œí’ˆ ì¸í„°í˜ì´ìŠ¤\n",
    "class Chair: pass\n",
    "class Table: pass\n",
    "\n",
    "# ìœˆë„ìš° ìŠ¤íƒ€ì¼ ì œí’ˆ\n",
    "class WindowsChair(Chair): pass\n",
    "class WindowsTable(Table): pass\n",
    "\n",
    "# ë§¥ ìŠ¤íƒ€ì¼ ì œí’ˆ\n",
    "class MacChair(Chair): pass\n",
    "class MacTable(Table): pass\n",
    "\n",
    "# ì¶”ìƒ íŒ©í† ë¦¬ ì¸í„°í˜ì´ìŠ¤\n",
    "class UIFactory:\n",
    "    def create_chair(self): pass\n",
    "    def create_table(self): pass\n",
    "\n",
    "# êµ¬ì²´ íŒ©í† ë¦¬\n",
    "class WindowsFactory(UIFactory):\n",
    "    def create_chair(self): return WindowsChair()\n",
    "    def create_table(self): return WindowsTable()\n",
    "\n",
    "class MacFactory(UIFactory):\n",
    "    def create_chair(self): return MacChair()\n",
    "    def create_table(self): return MacTable()\n",
    "\n",
    "# ì‚¬ìš©ìëŠ” íŒ©í† ë¦¬ë§Œ ë°›ê³  ë‚˜ë¨¸ì§€ëŠ” íŒ©í† ë¦¬ì— ë§¡ê¸´ë‹¤\n",
    "factory = WindowsFactory()\n",
    "chair = factory.create_chair()\n",
    "table = factory.create_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2c35fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ebc8e5f",
   "metadata": {},
   "source": [
    "#### ë°ì´í„° ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edac1d73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd42565e",
   "metadata": {},
   "source": [
    "#### Train ë°ì´í„° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f506db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ë°ì´í„° ë¶„í•  (ì‹œê³„ì—´ì´ë¯€ë¡œ shuffle=False)\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, shuffle=False)\n",
    "\n",
    "# 2. Train ë°ì´í„°ì—ì„œ ì´ìƒì¹˜ë¥¼ ì œì™¸í•œ 'Clean' ë°ì´í„° ìƒì„±\n",
    "# (ì˜ˆ: ìƒí•˜ìœ„ 1%ë¥¼ ì œê±°í•˜ê±°ë‚˜ RobustScalerì˜ ê¸°ì¤€ì´ ë˜ëŠ” IQR ë²”ìœ„ ë‚´ ë°ì´í„°ë§Œ ì‚¬ìš©)\n",
    "def clean_time_series_data(df, columns, multiplier=1.5):\n",
    "    \"\"\"\n",
    "    df: ë°ì´í„°í”„ë ˆì„\n",
    "    columns: ì´ìƒì¹˜ë¥¼ ì²´í¬í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ['f1', 'f2', 'f3'])\n",
    "    multiplier: IQR ê°€ì¤‘ì¹˜ (ë³´í†µ 1.5, ë” ì—„ê²©í•˜ê²Œ ì¡ìœ¼ë ¤ë©´ 1.0~1.2)\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    for col in columns:\n",
    "        # 1. IQR ê³„ì‚°\n",
    "        Q1 = df_clean[col].quantile(0.25)\n",
    "        Q3 = df_clean[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - multiplier * IQR\n",
    "        upper_bound = Q3 + multiplier * IQR\n",
    "        \n",
    "        # 2. ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ë°ì´í„°ë¥¼ NaNìœ¼ë¡œ ë³€ê²½\n",
    "        # (ë°”ë¡œ ì‚­ì œí•˜ì§€ ì•Šê³  NaNìœ¼ë¡œ ë§Œë“œëŠ” ê²ƒì´ í¬ì¸íŠ¸ì…ë‹ˆë‹¤)\n",
    "        outlier_mask = (df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)\n",
    "        outlier_count = outlier_mask.sum()\n",
    "        df_clean.loc[outlier_mask, col] = np.nan\n",
    "        \n",
    "        print(f\"[{col}] íƒì§€ëœ ì´ìƒì¹˜ ê°œìˆ˜: {outlier_count}\")\n",
    "\n",
    "    # 3. ë³´ê°„(Interpolation) ìˆ˜í–‰\n",
    "    # 'linear' ë°©ì‹ì€ ì¸ì ‘í•œ ë‘ ì •ìƒ ë°ì´í„° ì‚¬ì´ë¥¼ ì§ì„ ìœ¼ë¡œ ì—°ê²°í•©ë‹ˆë‹¤.\n",
    "    # 'limit_direction'ì€ ì–‘ë°©í–¥ìœ¼ë¡œ ì±„ì›Œì¤ë‹ˆë‹¤.\n",
    "    df_clean = df_clean.interpolate(method='linear', limit_direction='both')\n",
    "    \n",
    "    # ë§Œì•½ ì²« ë²ˆì§¸ í–‰ì´ë‚˜ ë§ˆì§€ë§‰ í–‰ì´ NaNì´ë¼ ë³´ê°„ë˜ì§€ ì•Šì€ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ \n",
    "    # ë‚¨ì€ NaNì€ ì•ë’¤ ê°’ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤.\n",
    "    df_clean = df_clean.ffill().bfill()\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# --- ì‹¤ì „ ì ìš© ì˜ˆì‹œ ---\n",
    "# 3ê°œ í”¼ì²˜ë¥¼ ê°€ì§„ ë°ì´í„°í”„ë ˆì„ì´ë¼ê³  ê°€ì • (train_data)\n",
    "feature_cols = train_data.columns.tolist() # í˜¹ì€ ['feat1', 'feat2', 'feat3']\n",
    "\n",
    "# ì´ìƒì¹˜ ë³´ê°„ ì²˜ë¦¬ëœ Clean ë°ì´í„° ìƒì„±\n",
    "train_clean = clean_time_series_data(train_data, feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d43f396",
   "metadata": {},
   "source": [
    "#### ë°ì´í„° ìŠ¤ì¼€ì¼ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11e6799",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RobustScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 1ì°¨ (RobustScaler): ì¤‘ì•™ê°’ê³¼ ì‚¬ë¶„ìœ„ìˆ˜ë¥¼ ì´ìš©í•´ ì´ìƒì¹˜ì˜ ì˜í–¥ì„ ìµœì†Œí™”í•˜ë©´ì„œ ë°ì´í„°ë¥¼ ëª¨ìë‹ˆë‹¤.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m scaler1 = \u001b[43mRobustScaler\u001b[49m()\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 2ì°¨ StandardScaler ì ìš©\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# scaler = StandardScaler()\u001b[39;00m\n\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# í•™ìŠµ ë°ì´í„°ë¡œ scaler fit\u001b[39;00m\n\u001b[32m      8\u001b[39m normal_train_reshaped = normal_train.reshape(-\u001b[32m1\u001b[39m, normal_train.shape[-\u001b[32m1\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'RobustScaler' is not defined"
     ]
    }
   ],
   "source": [
    "# 1. ìŠ¤ì¼€ì¼ëŸ¬ ì„ ì–¸ ë° \"Clean\" ë°ì´í„°ë¡œ í•™ìŠµ\n",
    "# RobustScalerëŠ” ê·¸ ìì²´ë¡œ ì´ìƒì¹˜ì˜ ì˜í–¥ì„ ëœ ë°›ìœ¼ë¯€ë¡œ ë§¤ìš° ì í•©í•©ë‹ˆë‹¤.\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# 2. 3ì°¨ì› -> 2ì°¨ì›ìœ¼ë¡œ ë³€í™˜ (reshape)\n",
    "# (Samples, Time_steps, Features) -> (Samples * Time_steps, Features)\n",
    "train_reshaped = train_clean.reshape(-1, 3)\n",
    "\n",
    "# 3. í•™ìŠµ ë° ë³€í™˜ (Transform)\n",
    "# Trainì€ ê¹¨ë—í•œ ìƒíƒœë¡œ ë³€í™˜í•˜ì—¬ í•™ìŠµì— ì‚¬ìš©\n",
    "scaler.fit(train_reshaped) # ì¤‘ìš”: ê¹¨ë—í•œ ë°ì´í„°ì˜ í†µê³„ê°’(ì¤‘ì•™ê°’, IQR)ì„ í•™ìŠµ\n",
    "train_scaled = scaler.transform(train_reshaped)\n",
    "\n",
    "# TestëŠ” ì´ìƒì¹˜ê°€ í¬í•¨ëœ 'Raw' ìƒíƒœ ê·¸ëŒ€ë¡œ ë³€í™˜\n",
    "# (í•™ìŠµëœ ì¤‘ì•™ê°’/IQR ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜ë˜ë¯€ë¡œ ì´ìƒì¹˜ëŠ” ë§¤ìš° í¬ê±°ë‚˜ ì‘ì€ ê°’ìœ¼ë¡œ ë‚¨ìŒ)\n",
    "test_scaled = scaler.transform(test_data)\n",
    "\n",
    "normal_train_scaled = train_scaled.reshape(train_clean.shape)\n",
    "normal_test_scaled = test_scaled.reshape(test.shape)\n",
    "\n",
    "print(\"ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ\")\n",
    "print(f\"Train mean: {normal_train_scaled.mean():.4f}, std: {normal_train_scaled.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bda3f85",
   "metadata": {},
   "source": [
    "#### Dataset ë° DataLoader ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e37e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = torch.FloatTensor(sequences)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx]\n",
    "\n",
    "# Dataset ìƒì„±\n",
    "train_dataset = TimeSeriesDataset(normal_train_scaled)\n",
    "val_dataset = TimeSeriesDataset(normal_val_scaled)\n",
    "test_dataset = TimeSeriesDataset(anomaly_test_scaled)\n",
    "\n",
    "# DataLoader ìƒì„±\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5d3c46",
   "metadata": {},
   "source": [
    "## 3. LSTM Autoencoder ëª¨ë¸ ì •ì˜\n",
    "LSTM AutoencoderëŠ” Encoder-Decoder êµ¬ì¡°ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤:  \n",
    " \n",
    "Encoder: ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ì ì¬ í‘œí˜„(latent representation)ìœ¼ë¡œ ì••ì¶•  \n",
    "Decoder: ì ì¬ í‘œí˜„ìœ¼ë¡œë¶€í„° ì›ë³¸ ì‹œí€€ìŠ¤ë¥¼ ì¬êµ¬ì„±  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5a353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout, seq_len):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.encoder_lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        # input_seq: (B, T, D)\n",
    "        encoder_outputs, (h_n, c_n) = self.encoder_lstm(input_seq)\n",
    "\n",
    "        # last hidden state -> latent representation\n",
    "        latent = h_n.squeeze(0)  # (B, H)\n",
    "        latent_seq = latent.unsqueeze(1).repeat(1, input_seq.size(1), 1)  # (B, T, H)\n",
    "\n",
    "        return latent_seq, encoder_outputs\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim, dropout, seq_len, use_output_activation):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.use_output_activation = use_output_activation\n",
    "\n",
    "        self.decoder_lstm = nn.LSTM(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, latent_seq):\n",
    "        # latent_seq: (B, T, H)\n",
    "        decoder_outputs, (h_n, c_n) = self.decoder_lstm(latent_seq)\n",
    "        reconstructed_seq = self.output_layer(decoder_outputs)  # (B, T, D)\n",
    "\n",
    "        if self.use_output_activation:\n",
    "            reconstructed_seq = self.activation(reconstructed_seq)\n",
    "\n",
    "        return reconstructed_seq, h_n\n",
    "\n",
    "\n",
    "class LSTMAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout, seq_len, use_output_activation=False):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            input_dim=input_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            dropout=dropout,\n",
    "            seq_len=seq_len\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            output_dim=input_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            dropout=dropout,\n",
    "            seq_len=seq_len,\n",
    "            use_output_activation=use_output_activation\n",
    "        )\n",
    "\n",
    "    def forward(self, input_seq, return_latent=False, return_encoder_outputs=False):\n",
    "        latent_seq, encoder_outputs = self.encoder(input_seq)\n",
    "        reconstructed_seq, last_hidden = self.decoder(latent_seq)\n",
    "\n",
    "        if return_latent:\n",
    "            return reconstructed_seq, last_hidden\n",
    "        elif return_encoder_outputs:\n",
    "            return reconstructed_seq, encoder_outputs\n",
    "\n",
    "        return reconstructed_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb2f88b",
   "metadata": {},
   "source": [
    "##### ëª¨ë¸ ì´ˆê¸°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd80ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "input_dim = 3  # feature ê°œìˆ˜\n",
    "hidden_dim = 64  # LSTM hidden dimension\n",
    "seq_len = 50  # sequence length\n",
    "dropout = 0.2\n",
    "\n",
    "# ëª¨ë¸ ìƒì„±\n",
    "model = LSTMAutoEncoder(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    dropout=dropout,\n",
    "    seq_len=seq_len,\n",
    "    use_output_activation=False\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7082b20a",
   "metadata": {},
   "source": [
    "## 4. í•™ìŠµ í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feba8ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        reconstructed = model(batch)\n",
    "        loss = criterion(reconstructed, batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            reconstructed = model(batch)\n",
    "            loss = criterion(reconstructed, batch)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def compute_reconstruction_error(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    ê° ìƒ˜í”Œì˜ ì¬êµ¬ì„± ì˜¤ì°¨ ê³„ì‚° (ì´ìƒ íƒì§€ìš©)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    errors = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            reconstructed = model(batch)\n",
    "            \n",
    "            # ìƒ˜í”Œë³„ MSE ê³„ì‚°\n",
    "            batch_errors = torch.mean((batch - reconstructed) ** 2, dim=(1, 2))\n",
    "            errors.extend(batch_errors.cpu().numpy())\n",
    "    \n",
    "    return np.array(errors)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bd4435",
   "metadata": {},
   "source": [
    "## 5. ëª¨ë¸ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3bd664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ì„¤ì •\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 50\n",
    "\n",
    "# í•™ìŠµ ì´ë ¥ ì €ì¥\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"ëª¨ë¸ í•™ìŠµ ì‹œì‘...\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        print(f\"  Train Loss: {train_loss:.6f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "print(\"\\ní•™ìŠµ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bb79b3",
   "metadata": {},
   "source": [
    "#### í•™ìŠµ ê³¡ì„  ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8858ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('LSTM Autoencoder Training Curves')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be961aeb",
   "metadata": {},
   "source": [
    "## 6. ì´ìƒ íƒì§€ ì„±ëŠ¥ í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f22fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¬êµ¬ì„± ì˜¤ì°¨ ê³„ì‚°\n",
    "print(\"ì¬êµ¬ì„± ì˜¤ì°¨ ê³„ì‚° ì¤‘...\")\n",
    "normal_errors = compute_reconstruction_error(model, val_loader, device)\n",
    "anomaly_errors = compute_reconstruction_error(model, test_loader, device)\n",
    "\n",
    "print(f\"ì •ìƒ ë°ì´í„° ì¬êµ¬ì„± ì˜¤ì°¨ - Mean: {normal_errors.mean():.6f}, Std: {normal_errors.std():.6f}\")\n",
    "print(f\"ì´ìƒ ë°ì´í„° ì¬êµ¬ì„± ì˜¤ì°¨ - Mean: {anomaly_errors.mean():.6f}, Std: {anomaly_errors.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfb1923",
   "metadata": {},
   "source": [
    "#### ì¬êµ¬ì„± ì˜¤ì°¨ ë¶„í¬ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9036a9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# íˆìŠ¤í† ê·¸ë¨\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(normal_errors, bins=30, alpha=0.7, label='Normal', color='blue')\n",
    "plt.hist(anomaly_errors, bins=30, alpha=0.7, label='Anomaly', color='red')\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Reconstruction Error Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# ë°•ìŠ¤í”Œë¡¯\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot([normal_errors, anomaly_errors], labels=['Normal', 'Anomaly'])\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.title('Reconstruction Error Comparison')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73e1e63",
   "metadata": {},
   "source": [
    "#### ì„ê³„ê°’ ê¸°ë°˜ ì´ìƒ íƒì§€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94b9552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„ê³„ê°’ ì„¤ì • (ì •ìƒ ë°ì´í„° í‰ê·  + 2*í‘œì¤€í¸ì°¨)\n",
    "threshold = normal_errors.mean() + 2 * normal_errors.std()\n",
    "print(f\"ì´ìƒ íƒì§€ ì„ê³„ê°’: {threshold:.6f}\")\n",
    "\n",
    "# ì´ìƒ íƒì§€ ìˆ˜í–‰\n",
    "normal_predictions = (normal_errors > threshold).astype(int)\n",
    "anomaly_predictions = (anomaly_errors > threshold).astype(int)\n",
    "\n",
    "# ì„±ëŠ¥ í‰ê°€\n",
    "normal_accuracy = 1 - normal_predictions.mean()\n",
    "anomaly_accuracy = anomaly_predictions.mean()\n",
    "\n",
    "print(f\"\\nì •ìƒ ë°ì´í„° ì •í™•ë„: {normal_accuracy:.2%}\")\n",
    "print(f\"ì´ìƒ ë°ì´í„° íƒì§€ìœ¨: {anomaly_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d49cf9",
   "metadata": {},
   "source": [
    "## 7. ì¬êµ¬ì„± ê²°ê³¼ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3688800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì •ìƒ ë°ì´í„° ì¬êµ¬ì„± ì˜ˆì‹œ\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_normal = torch.FloatTensor(normal_val_scaled[0:1]).to(device)\n",
    "    reconstructed_normal = model(sample_normal).cpu().numpy()[0]\n",
    "\n",
    "    sample_anomaly = torch.FloatTensor(anomaly_test_scaled[0:1]).to(device)\n",
    "    reconstructed_anomaly = model(sample_anomaly).cpu().numpy()[0]\n",
    "\n",
    "# ì‹œê°í™”\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# ì •ìƒ ë°ì´í„°\n",
    "for i in range(3):\n",
    "    axes[0, i].plot(normal_val_scaled[0, :, i], label='Original', linewidth=2)\n",
    "    axes[0, i].plot(reconstructed_normal[:, i], label='Reconstructed', linestyle='--', linewidth=2)\n",
    "    axes[0, i].set_title(f'Normal - Feature {i+1}')\n",
    "    axes[0, i].set_xlabel('Time')\n",
    "    axes[0, i].set_ylabel('Value')\n",
    "    axes[0, i].legend()\n",
    "    axes[0, i].grid(True)\n",
    "\n",
    "# ì´ìƒ ë°ì´í„°\n",
    "for i in range(3):\n",
    "    axes[1, i].plot(anomaly_test_scaled[0, :, i], label='Original', linewidth=2, color='red')\n",
    "    axes[1, i].plot(reconstructed_anomaly[:, i], label='Reconstructed', linestyle='--', linewidth=2, color='orange')\n",
    "    axes[1, i].set_title(f'Anomaly - Feature {i+1}')\n",
    "    axes[1, i].set_xlabel('Time')\n",
    "    axes[1, i].set_ylabel('Value')\n",
    "    axes[1, i].legend()\n",
    "    axes[1, i].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81e20a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
